# -*- coding: utf-8 -*-
"""Youtube threat classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_c-ecVeojn79aONLHnvwry0MS-1NRZqw

*italicized text*

Run the model only on collab 

Hierarchial attention network uses GPU to train and hence we have trained the model on Collab.

before running the code make sure that you hve turned the GPU ON. To do so go to RUNTIME and then CHANGE RUNTIME TYPES. In that switch the Hardware accelerator to use GPU.

In order to confirm that GPU is turned on the below code should return  '/device:GPU:0'
"""

import tensorflow as tf
tf.test.gpu_device_name()

#installing requirements for downloading files from drive
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#google drive link for the cleaned, labelled pre processed youtube data
link='https://drive.google.com/open?id=1SAZ5_KrNHQe8FSsdrBQQGTF61yKUmXuW'

fluff, id = link.split('=')
print (id)

import pandas as pd

Sometime the below lines get buggy. I case you get an error , please run the cell again!

#downloading the data from the link
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('labelled_processed_youtube_data.csv')

df = pd.read_csv('labelled_processed_youtube_data.csv').drop(["Unnamed: 0"],axis=1)

df.shape

#creating a feature
df['ratio']=df['dislikes']/df['likes']

df.shape

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use("fivethirtyeight")

import os

import os
import time
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from tqdm import tqdm
import math
from sklearn.model_selection import train_test_split
from sklearn import metrics

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D
from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate
from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D
from keras.optimizers import Adam
from keras.models import Model
from keras import backend as K
from keras.engine.topology import Layer
from keras import initializers, regularizers, constraints, optimizers, layers

def split_text(x):
    x = wordninja.split(x)
    return '-'.join(x)

df["merged"] = df["merged"].str.lower()
  print("Train shape : ",df.merged.shape)

df=df.reindex(np.random.permutation(df.index))

df['ratio']=df['ratio'].fillna(0)

#train test split
train_df, test_df = train_test_split(df, test_size=0.2, random_state=2018)
#train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=2018)
print(train_df.shape)
print(test_df.shape)

## fill up the missing values
train_X = train_df["merged"].fillna("_##_").values
#val_X = val_df["merged"].fillna("_##_").values
test_X = test_df["merged"].fillna("_##_").values

#defining the embeddings and other parameters
  embed_size = 300 # how big is each word vector
  max_features = 36156 # how many unique words to use (i.e num rows in embedding vector)
  maxlen = 500

#tokenising the text data
tokenizer = Tokenizer(num_words=max_features)
a=tokenizer.fit_on_texts(list(df['merged']))
train_X = tokenizer.texts_to_sequences(train_X)
#val_X = tokenizer.texts_to_sequences(val_X)
test_X = tokenizer.texts_to_sequences(test_X)

## Pad the sentences 
train_X = pad_sequences(train_X, maxlen=maxlen)
#val_X = pad_sequences(val_X, maxlen=maxlen)
test_X = pad_sequences(test_X, maxlen=maxlen)

## Get the target values
train_y = train_df['label'].values
test_y = test_df['label'].values

# wiki_news embeddings
embedding_link1= 'https://drive.google.com/open?id=1i1EVafb-d4SrdQYT_PpLIYq2A2X25Ac_'
# paragram embeddings
embedding_link2= 'https://drive.google.com/open?id=1nV0gO-d6GIrlr_0RRD1k1ZlekOfv9wSi'
# glove embeddings
embedding_link3= 'https://drive.google.com/open?id=1aRfxfzT1UXuo_qbg_J3_GG8HsbL7L4gr'

fluff, id = embedding_link1.split('=')
print (id)

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('wiki-news-300d-1M.vec')

fluff, id = embedding_link2.split('=')
print (id)

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('paragram_300_sl999.txt')

fluff, id = embedding_link3.split('=')
print (id)



downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('glove.840B.300d')

#The size of the embeddings are very large.  It is adviced to use only one ebedding in one instance for making sure that the code runs properly.

#creating respective embeddings


def load_fasttext(word_index):    
    EMBEDDING_FILE = 'wiki-news-300d-1M.vec'
    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE,encoding="utf8") if len(o)>100)

    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = all_embs.mean(), all_embs.std()
    embed_size = all_embs.shape[1]

    # word_index = tokenizer.word_index
    nb_words = min(max_features, len(word_index)+1)
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= max_features: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector

    return embedding_matrix
  
def load_glove(word_index):
    EMBEDDING_FILE = 'glove.840B.300d'
    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE))

    all_embs = np.stack(embeddings_index.values()+1)
    emb_mean,emb_std = -0.005838499,0.48782197
    embed_size = all_embs.shape[1]

    # word_index = tokenizer.word_index
    nb_words = min(max_features, len(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= max_features: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector
            
    return embedding_matrix 


def load_para(word_index):
    EMBEDDING_FILE = 'paragram_300_sl999.txt'
    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE, encoding="utf8", errors='ignore') if len(o)>100)

    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = -0.0053247833,0.49346462
    embed_size = all_embs.shape[1]
    print(emb_mean,emb_std,"para")

    # word_index = tokenizer.word_index
    nb_words = min(max_features, len(word_index)+1)
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= max_features: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector
    
    return embedding_matrix

#creating attention layers

class Attention(Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight((input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight((input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),
                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))

        if self.bias:
            eij += self.b

        eij = K.tanh(eij)

        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim

#creating cnn model
def model_cnn(embedding_matrix):
    filter_sizes = [1,2,3,5]
    num_filters = 36

    inp1 = Input(shape=(maxlen,))
    inp2 = Input(shape=(1,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp1)
    x = Reshape((maxlen, embed_size, 1))(x)

    maxpool_pool = []
    for i in range(len(filter_sizes)):
        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),
                                     kernel_initializer='he_normal', activation='elu')(x)
        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))

    z = Concatenate(axis=1)(maxpool_pool)   
    z = Flatten()(z)
    z = Dropout(0.1)(z)

    y = Dense(64, activation="relu")(inp2)
    y = Dense(32, activation="relu")(y)
    y = Dense(4, activation="relu")(y)


    conc = concatenate([z,y])

    conc = Dense(64, activation="relu")(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="sigmoid")(conc)    


    model = Model(inputs=[inp1,inp2], outputs=outp)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

#creating lstm models
def model_lstm_atten_2(embedding_matrix):
    inp1 = Input(shape=(maxlen,))
    inp2 = Input(shape=(1,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp1)
    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)
    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)
    x = Attention(maxlen)(x)
    x = Dense(64, activation="relu")(x)
    
    y = Dense(64, activation="relu")(inp2)
    y = Dense(32, activation="relu")(y)
    y = Dense(4, activation="relu")(y)
    
    conc = concatenate([x,y])
    
    conc = Dense(64, activation="relu")(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="sigmoid")(conc)

    model = Model(inputs=[inp1,inp2], outputs=outp)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

#building gru with attention network model
def model_gru_ap_atten(embedding_matrix):
    inp1 = Input(shape=(maxlen,))
    inp2 = Input(shape=(1,))
    
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp1)
    #(cuDNN) is a GPU-accelerated library of primitives for deep neural networks
    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)
    x = Attention(maxlen)(x) # New
    x = Dense(16, activation="relu")(x)
    x = Dropout(0.1)(x)
    
    y = Dense(64, activation="relu")(inp2)
    y = Dense(32, activation="relu")(y)
    y = Dense(4, activation="relu")(y)
    
    conc = concatenate([x,y])
    
    
    conc = Dense(64, activation="relu")(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="sigmoid")(conc)

    model = Model(inputs=[inp1,inp2], outputs=outp)
    
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

#building gru with attention layers
def model_gru_atten_3(embedding_matrix,):
  
    inp1 = Input(shape=(maxlen,))
    inp2 = Input(shape=(1,))

    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp1)
    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)
    x = Bidirectional(CuDNNGRU(100, return_sequences=True))(x)
    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)
    x = Attention(maxlen)(x)

    y = Dense(64, activation="relu")(inp2)
    y = Dense(32, activation="relu")(y)
    y = Dense(4, activation="relu")(y)

    conc = concatenate([x,y])

    conc = Dense(64, activation="relu")(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="sigmoid")(conc)

    model = Model(inputs=[inp1,inp2], outputs=outp)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

#building lstm model
  def model_lstm_du(embedding_matrix):
    inp1 = Input(shape=(maxlen,))
    inp2 = Input(shape=(1,))
    
    #1st input
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp1)
    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    
    #2nd input
    y = Dense(64, activation="relu")(inp2)
    y = Dense(32, activation="relu")(y)
    y = Dense(4, activation="relu")(y)
   # y = Model(inputs=inp2, outputs=outp)#you can add y also in conc removing model

    conc = concatenate([avg_pool, max_pool,y])
    
    conc = Dense(64, activation="relu")(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="sigmoid")(conc)
    
    model = Model(inputs=[inp1,inp2], outputs=outp)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    #model.summary()
    return model

INPUT2=train_df['ratio'].as_matrix()

#training model and predicting
def train_pred(model, epochs):
     
      history=model.fit([train_X,train_df['ratio']], train_y, batch_size=52,epochs=epochs,verbose=1,validation_split=.2)
      #model.save_weights('model_weights.h5')

# Save the model architecture
      #with open('model_architecture.json', 'w') as f:
       #   f.write(model.to_json())
    #pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)
      pred_test_y = model.predict([test_X], batch_size=128, verbose=0)
    #Plot training & validation accuracy values
      plt.plot(history.history['acc'])
      plt.plot(history.history['val_acc'])
      plt.title('Model accuracy')
      plt.ylabel('Accuracy')
      plt.xlabel('Epoch')
      plt.legend(['Train', 'Test'], loc='upper left')
      plt.show()

      # Plot training & validation loss values
      plt.plot(history.history['loss'])
      plt.plot(history.history['val_loss'])
      plt.title('Model loss')
      plt.ylabel('Loss')
      plt.xlabel('Epoch')
      plt.legend(['Train', 'Test'], loc='upper left')
      plt.show()
      pred_test_y = model.predict([test_X], batch_size=128, verbose=0)
      return pred_test_y

word_index=tokenizer.word_index

vocab = []
for w,k in word_index.items():
    vocab.append(w)
    if k >= max_features:
        break

#creating embedding matrix
embedding_matrix_1 = load_glove(word_index)

#You can save the embedding matrix so that it can be reused for future purposes and also to reduce the time it takes to build the embeddings from scratch

#np.save("embedding_matrix_1.npy",embedding_matrix_1)

#creating embedding matrix
embedding_matrix_2 = load_fasttext(word_index)

#np.save("embedding_matrix_2.npy",embedding_matrix_2)

#creating embedding matrix
embedding_matrix_3 = load_para(word_index)

#np.save("embedding_matrix_3.npy",embedding_matrix_3)

#creating a mean embedding matrix

embedding_matrix = np.mean([embedding_matrix_1,embedding_matrix_2,embedding_matrix_3], axis = 0)
np.shape(embedding_matrix)

#Model CNN 
#Embedding GLOVE
pred_test_y=train_pred(model_cnn(embedding_matrix_1), epochs = 15)

from keras.models import load_model

#Model CNN 
#Embedding FastText
train_pred(model_cnn(embedding_matrix_2), epochs = 10)

#Model CNN 
#Embedding Paragram
train_pred(model_cnn(embedding_matrix_3), epochs = 15)

#Model CNN 
#Embedding Combined
train_pred(model_cnn(embedding_matrix), epochs = 15)

#Model CNN 
#Embedding GLOVE
train_pred(model_lstm_atten_2(embedding_matrix_1), epochs = 15)

#Model CNN 
#Embedding FastText
train_pred(model_lstm_atten_2(embedding_matrix_2), epochs = 15)

#Model CNN 
#Embedding Paragram
train_pred(model_lstm_atten_2(embedding_matrix_3), epochs = 15)

#Model CNN 
#Embedding Combined
train_pred(model_lstm_atten_2(embedding_matrix), epochs = 15)

#Model CNN 
#Embedding GLOVE
train_pred(model_gru_ap_atten(embedding_matrix_1), epochs = 15)

#Model CNN 
#Embedding FastText
train_pred(model_gru_ap_atten(embedding_matrix_2), epochs = 15)

#Model CNN 
#Embedding Paragram
train_pred(model_gru_ap_atten(embedding_matrix_3), epochs = 15)

#Model CNN 
#Embedding Combined
train_pred(model_gru_ap_atten(embedding_matrix), epochs = 15)

#Model CNN 
#Embedding GLOVE
train_pred(model_lstm_du(embedding_matrix_1), epochs = 15)

#Model CNN 
#Embedding FastText
train_pred(model_lstm_du(embedding_matrix_2), epochs = 15)

#Model CNN 
#Embedding Paragram
train_pred(model_lstm_du(embedding_matrix_3), epochs = 15)

#Model CNN 
#Embedding Combined
train_pred(model_lstm_du(embedding_matrix), epochs = 15)

#Model CNN 
#Embedding GLOVE
train_pred(model_gru_atten_3(embedding_matrix_1), epochs = 150)

#Model CNN 
#Embedding FastText
train_pred(model_gru_atten_3(embedding_matrix_2), epochs = 15)

#Model CNN 
#Embedding Paragram
train_pred(model_gru_atten_3(embedding_matrix_3), epochs = 15)

#Model CNN 
#Embedding Combined
train_pred(model_gru_atten_3(embedding_matrix), epochs = 15)

def train_pred_new(model, epochs):
    history=model.fit([train_X,train_df['ratio']], train_y, batch_size=8,epochs=epochs,verbose=1,validation_split=.2)
    pred_test_y = model.predict([test_X,test_df['ratio']], batch_size=1024, verbose=0)
    #pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)
    #Plot training & validation accuracy values
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

    # Plot training & validation loss values
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()
    return pred_test_y #pred_val_y, pred_test_y

pred_test_y = train_pred+_new(model_gru_atten_3(embedding_matrix), epochs = 15)

pred_val_y = model.predict([test_X], batch_size=1024, verbose=0)

pred_test_y = (pred_test_y > 0.34).astype(int)

from sklearn.metrics import accuracy_score
print(accuracy_score(test_y, pred_test_y))